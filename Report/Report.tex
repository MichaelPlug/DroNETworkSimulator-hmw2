\documentclass[12pt]{article}
\usepackage{graphicx}

\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{float}

\usepackage[T1]{fontenc} 							% imposta la codifica dei font
\usepackage[utf8]{inputenc} 							% lettere accentate da tastiera
\usepackage[italian]{babel} 							% per scrivere in italiano



\usepackage{footmisc}


\usepackage{fancyhdr}
\usepackage{longtable}


\usepackage{subfig}


\usepackage{listings}


\definecolor{mGreen}{rgb}{0,0.6,0}
\definecolor{mGray}{rgb}{0.5,0.5,0.5}
\definecolor{mPurple}{rgb}{0.58,0,0.82}
\definecolor{backgroundColour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{CStyle}{
    backgroundcolor=\color{backgroundColour},   
    commentstyle=\color{mGreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{mGray},
    stringstyle=\color{mPurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    language=C
}

\addtolength{\headheight}{1.5cm} % make more space for the header
\pagestyle{fancyplain} % use fancy for all pages except chapter start
\lhead{\includegraphics[height=0.1cm]{./Images/logo_hidden.png}} % left logo
\rhead{\includegraphics[height=2.3cm]{./Images/logo.png}} % right logo
\renewcommand{\headrulewidth}{0pt} % remove rule below header

\usepackage[colorinlistoftodos]{todonotes}

\newcommand\tab[1][1cm]{\hspace*{#1}}

\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE Universita' La Sapienza}\\[1.5cm] % Name of your university/college
\textsc{\Large Dipartimento di Informatica}\\[0.5cm] % Major heading such as course name
\textsc{\large Autonomous Networking}\\[0.5cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries Report Secondo Homework}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Giordano \textsc{Dionisi} 1834919 \\

Mattia \textsc{Lisi} 1709782 \\

Michele \textsc{Spina} 1711821

\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Prof. Andrea \textsc{Coletta} \\% Supervisor's Name
Prof.ssa Gaia \textsc{Maselli} % Supervisor's Name

\end{flushright}
\end{minipage}\\[2cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

\includegraphics[height=3.8cm]{./Images/logo_firstpage.png}\\[3cm] % Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace


\end{titlepage}

\tableofcontents

\newpage

\section{Assunzioni}

Si assume che sia stato letto e compreso il primo report. 

Tutti gli approcci non soddisfacenti, nel primo Homework, non sono stati considerati visto che il secondo Homework è un'estenzione del primo: tipo l'algoritmo NONE, UCB e via dicendo non sono stati riconsiderati perchè ovviamente continuerebbero a mantenere pessime performance, naturalmente

\section{Starting Point}

Consideriamo il miglior algoritmo utilizzato in precedenza: l'AISG.

Il nostro algoritmo competitor è l'MGEO: esso lavora come il Geographical Algorithm, solo che nel caso non ci siano dei nodi vicini per un determinato drone, allora si ha l'azione di tornare al depot

Tale azione è molto energy-consuming, anche se porta grandi risultati.


\subsection{Approccio Epsilon-Greedy}

Tutti gli algoritmi che vedremo utilizzano l'approccio Epsilon-Greedy, visto che in precedenza ha portato ottimi risultati. 

In questo caso si ha che con probabilità:

\begin{itemize}

    \item 1-$\epsilon$ si sfrutta il Q-Learning, 
    
    \item $\epsilon$ si sfrutta l'MGEO modificato per aggiornare i valori del Q-Learning\footnote{Non si sfrutta l'approccio randomico visto che in precedenza è stato scadente}. 
    
\end{itemize}


\subsubsection{Ritorno dell'OIV}

Precedentemente l'OIV\footnote{Optimistic Initial Value} aveva pessimi risultati, ma lo si è risperimentato perchè in questo caso il problema è leggermente diverso vista la nuova azione introdotta.

Sperimentalmente un OIV = 10 comporta una successiva migliore decisione sub-ottimale. Infatti si ha un'esplorazione iniziale maggiore\footnote{ovviamente la funzione di Reward è la stessa del primo Homework, quindi il valore massimo che si può prendere è pari a 2} e quindi il learning per il K-Armed Bandit/Q-Learning è sensibilmente migliore. In precedenza non avveniva ciò perchè non vi era anche l'azione del ritorno al depot, quindi non c'erano grandi influenze per ciò.

\section{Cosa fare? Migliorare l'Esistente}

\subsection{Da AISG ad AISG\_Updated}

L'AISG è un approccio che non tiene in considerazione il rientro al depot, quindi si è realizzato un AISG leggermente migliore: quando non ci sono nodi vicini allora il nodo stesso torna al depot.

Solo con questa modifica si ha:

\begin{enumerate}
    \item Un Battery-Consuming maggiore $\rightarrow$ Infatti frequentemente si torna al depot e tale azione consuma molto;
    
    \item Uno Score abbastanza concorrente ad MGEO $\rightarrow$ Questo ci fa ben sperare visto che si sta sfruttando un semplice approccio K-Armed Bandit e non Q-Learning
\end{enumerate}

Con tutto ciò si ha un ottimo punto di partenza per i successivi algoritmi.

\begin{figure}[H]
    
    \subfloat{{\includegraphics[width=8.1cm,height=\textheight,keepaspectratio]{./Images/AISG_AISGUPDATED_MGEO_GEO_score.jpg} }}%
    \qquad
    \subfloat{{\includegraphics[width=8.1cm,height=\textheight,keepaspectratio]{./Images/AISG_AISGUPDATED_MGEO_GEO_energy.jpg} }}%
    \caption{AISG vs AISG\_UPDATED vs MGEO vs GEO}%
    \label{fig:example}%
\end{figure}


Si è inserito anche il Geographical Routing per capire il suo livello di prestazioni $\rightarrow$ Come l'AISG anche il GEO non considera l'opzione di tornare al depot e questo lo esclude da molte possibili scelte $\rightarrow$ Perciò ha un andamento chiaramente inferiore rispetto all'MGEO ed all'AISG\_Updated

\subsection{Problemi..?}

Purtroppo anche l'AISG Updated è lontano dall'MGEO, sopratutto a livello di score $\rightarrow$ Ciò è spiegabile perchè con il K-Armed Bandit\footnote{Tipologia di Reinforcement Learning utilizzata in AISG Updated} si hanno azioni ed un solo stato per il sistema, quindi non si sa cosa è accaduto in precedenza (nessun concetto di storia) $\rightarrow$ Ciò crea dei problemi, perchè in tale approccio si ha un learning molto limitato e nel problema in cui i droni quasi mai tornano al depot, allora i problemi possono aumentare esponenzialmente.
 
Quindi si passa al Q-Learning, che è un'estensione del K-Armed Bandit $\rightarrow$ Esso sfrutta gli stati e la storia passata ed infatti, nel nostro problema si adatta benissimo portando risultati sensibilmente migliori.

\section{Primi Approcci al Q-Learning}

\subsection{Che decisioni prendere? $\rightarrow$ 2 Actions}


Ora il problema è: Come implementare una base di Q-Learning? Supponiamo di avere $\mu$ droni, l'idea più banale è avere $\mu$ stati e sole due azioni. 

Per ogni stato\footnote{Ovvero drone, come detto} si hanno due possibili azioni:

\begin{enumerate}

    \item Tenere il pacchetto per sè;
    
    \item Passare il pacchetto ad un qualsiasi vicino.
    
\end{enumerate}

Grande problema: non sfrutta il ritorno al depot $\rightarrow$ Ciò comporta che il suo consumo energetico è bassissimo, ma tantissimi pacchetti scadono, sopratutto quando si hanno pochi droni. 

Ricorda: il nostro obiettivo principale è lo score piuttosto che il consumo energetivo $\rightarrow$ Si vuole che vengano consegnati più pacchetti possibili e per tale scopo tale approccio non è soddisfacente.

\subsubsection{Torniamo Anche al Depot: 3 Actions}

Consideriamo un'azione in più: \textbf{Tornare al Depot.}

Il consumo energetico aumenta, ma le prestazioni hanno un netto miglioramento $\rightarrow$ Purtroppo si è ancora troppo distanti rispetto alle prestazioni dell'MGEO

\subsubsection{Possibile Tentativo: N+1 Actions}

E se le azioni possibili non fossero solo 3 ma n+1, ovvero:

\begin{itemize}

    \item Tenere il pacchetto;
    \item Andare al Depot;
    \item Passarlo ad uno degli n-1 vicini\footnote{Non obbligatoriamente tutti contemporaneamente visibili in un certo istante.}
    .
\end{itemize}

Problema: non converge mai $\rightarrow$ Non riesce mai ad imparare abbastanza e ciò comporta che a fine simulazione ancora non si è arrivati ad avere delle scelte significative, perchè l'algoritmo sta ancora un pò in fase di esplorazione $\rightarrow$ Ciò spiega le disastrose prestazioni, rispetto agli altri algoritmi.

    
\subsection{Ulteriore Tentativo: Approccio vicino al Depot}

Se si è entro un certo range dal depot e si ha almeno un pacchetto, allora si va verso il depot, se non ci sono altre possibilità. 

Si rientra al depot se non si consuma eccessivamente $\rightarrow$ Se si è dall'altra parte dello scenario, allora si consuma troppa energia e si evita. In tal caso si aspetta un drone vicino o si mantiene il pacchetto.

Chiaramente questa modifica non può fornire risultati buoni, perchè può succedere che un drone molto lontano dal depot non ha vicini e quindi tutti i pacchetti da lui generati continuano a morire. Lo score è il nostro obiettivo, quindi questo è un tentativo fallito.


\section{Svolta: 3A\_TMGEO}

Le seguenti migliorie generano ottimi risultati. Definiamo $\xi$ il nodo attuale, $\zeta$ il suo buffer e $\eta$ i pacchetti, $\alpha$ un eventuale suo vicino:

\begin{itemize}

    \item Se in $\zeta$ ci sono paccheti che stanno per morire, allora $\xi$ torna verso il depot, per evitare che $\eta$ vengano persi e quindi che la Deliveery Ratio aumenti sensibilmente (così lo Score);

    \item Se $\xi$ ha un pacchetto ed $\alpha$ sta andando verso il depot, allora $\xi$ gli passa il pacchetto, perchè arriveranno molto velocemente, rispetto a cercare delle trasmissioni eventuali.
    
    \item  Se $\xi$ ha un pacchetto ed $\alpha$ è in unos tato di \textit{GoMustBack}, allora $\xi$ gli passa il pacchetto, perchè con molta probabilità arriveranno molto velocemente, rispetto a cercare delle trasmissioni eventuali.
    
    \item $\xi$ deve ritornare la depot, esse entra in uno stato di \textit{GoMustBack}\footnote{Non è uno stato del sistema, quindi per il Q-Learning, ma uno stato personale del drone}.
    Un drone in uno stato di \textit{GoMustBack} prosegue lungo la propria traiettoria fintanto che si sta avvicinando al depot (esse è possibile osservarlo calcolando l'angolo tra la traiettoria del drone e la traiettoria che dovrebbe percorrere per otrnare al depot), in caso contrario torna al depot. Tramite questo approccio si massimizzano gli spostamenti utili del drone, minimizzando la lunghezza dei percorsi da e verso il depot.
 \textit{GoMustBack} o fare la scelta migliore secondo il Q-learning.
    
\end{itemize}

\begin{figure}[H]
    
    \subfloat{{\includegraphics[width=8.1cm,height=\textheight,keepaspectratio]{./Images/3ATMGEO_3AMGEO_MGEO_energy.jpg} }}%
    \qquad
    \subfloat{{\includegraphics[width=8.1cm,height=\textheight,keepaspectratio]{./Images/3ATMGEO_3AMGEO_MGEO_score.jpg} }}%
    \caption{3A\_TMGEO vs 3A\_MGEO vs MGEO}%
    \label{fig:example}%
\end{figure}


\subsection{Novità: Grid Algorithm}

Considerare i droni come stati comporta una difficile convergenza, perchè si hanno troppi stati, generalmente. Tale strategia non tiene in considerazione la posizione di un drone ed il passaggio di stato dipende dal passaggio da un drone all'altro per il pacchetto.

Ma non è importante il drone come soggetto, perchè è importante la sua posizione $\rightarrow$ Se due droni si trovano nella stessa posizione, allora effettivamente essi sono equivalenti.

Nel Grid Algorithm si divide lo scenario in $\beta$ parti ed ogni parte corrisponde ad uno stato. Tutte le migliorie che funzionano nel 3A\_TGEO funzionano anche qui, perchè esse non dipendono dal design usato.

L'unica azione diversa è il \textit{passaggio ai droni vicini:} non si considerano i droni, quindi si passa il pacchetto ai droni nella cella adiacente (si prende il miglior drone di quella cella)


\subsubsection{Miglioria: Grid\_WGEO}

Il Grid Algorithm si utilizza come chiave per il Q-SET l'indice della cella per lo stato, ma tale approccio è debole perchè non considera la direzione dei droni.

Il Grid\_WGEO sfrutta sia l'informazione della propria cella e sia l'informazione della cella che verrà attraversata dalla sua traiettoria. In questo modo si hanno delle migliorie, perchè si considera anche la direzione di dove si va, non si considera però il next\_target per evitare una mancata convergenza (si è provato anche tale approccio ma è stato fallimentare per questo problema), quindi si ha una convergenza abbastanza veloce poiché ogni cella ha dalle 2 alle 4 celle confinanti.


\begin{figure}[H]
    
    \subfloat{{\includegraphics[width=8.1cm,height=\textheight,keepaspectratio]{./Images/GRIDWGEO_MGEO_3ATMGEO_energy.jpg} }}%
    \qquad
    \subfloat{{\includegraphics[width=8.1cm,height=\textheight,keepaspectratio]{./Images/GRIDWGEO_MGEO_3ATMGEO_score.jpg} }}%
    \caption{GRID\_WGEO vs MGEO vs 3A\_TMGEO}%
    \label{fig:example}%
\end{figure}


Analizziamo i risultati step-by-step:

\begin{itemize}

    \item A livello di score è palese che l'andamento è estremamente vicino all'MGEO, quando non si hanno tanti droni $\rightarrow$ l'MGEO rientra spesso al depot, mentre GRID\_WGEO e 3A\_TMGEO sfruttano il Q-Learning; 
    
    Considerare come stati le celle e non i droni comporta un sensibile miglioramento $\rightarrow$ Ciò perchè due droni che si trovano nella stessa posizione sono equivalenti $\rightarrow$ Si hanno risultati migliori perchè si riesce a convergere e quindi si riesce ad imparare molto più velocemente e precisamente;
    
    \item A livello di energia si hanno risultati migliori del GRID\_WGEO rispetto al 3A\_TMGEO, visto il diverso concetto di \textit{stato} e perchè si impara più velocemente considerando meno frequentemente l'opzione di ritorno al depot.
    
    Ma è ancora decisamente lineare con la crescita del numero dei droni rispetto all'MGEO, perchè si torna frequentemente al depot anche se ci sono molti vicini, solo perchè iol Q-Learning dice che è l'azione ottimale, mentre è sub-ottimale.

\end{itemize}


\section{Sviluppi futuri sulla batteria}

Nel corso di questa ricerca si è svolta anche un'indagine su come ridurre al minimo il consumo della batteria senza perdita di score, o addirittura migliorandolo.

Si è scelto di ridurre il numero di ritorni facendo convogliare i pacchetti su un unico drone quando due o più droni aventi un pacchetto si incontrano $rightarrow$ Sono stati utilizzati due diversi approcci: 

\begin{itemize}

    \item Convogliando i pacchetti sul drone più vicino al depot;
    
    \item Scegliendo il drone seguendo ai valori del Q-SET.
    
\end{itemize}

Questi approcci hanno dato risultati interessanti, abbasando notevolemente il consumo di batteria ottenendo però uno score simile agli algoritmi oroginali $\rightarrow$ Sarebbe interessante approfondire in futuro questa ricerca.


\section{Sezioni Implementate per Componenti}

\begin{itemize}

    \item \textbf{Giordano:} E' il principale responsabile della costruzione e realizzazione dei seguenti algoritmi:
    
    \begin{itemize}
        
        \item AISG\_Updated
        
        \item 3A\_MGEO
        
        \item 2\_Actions
        
        \item N+1 Actions
        
        \item Grid Algorithm. 
        
    \end{itemize}
    
    Ha inoltre provveduto in maniera principale alla stesura e realizzazione del report.
    
    \item \textbf{Mattia:} Ha collaborato nelle attività di ricerca sugli sviluppi futuri.
    
    \item \textbf{Michele:} Ha ideato e realizzato i seguenti algoritmi:
        \begin{itemize}
        
		\item	GRID\_W\_NEXT\_UP
	
		\item	TMGEO
		 
	\end{itemize}
		Ha collaborato nella realizzazione degli altri algoritmi e del report, realizzato i plot e svolto le attività di ricerca sugli sviluppi futuri
    
    
\end{itemize}

\section{Conclusioni}

Per questo Homework il miglior algoritmo realizzato è il GRID\_WGEO per tutto ciò che è stato già largamente spiegato.

Di seguito un riassunto di tutti gli algoritmi proposti con le varie performance (a livello di score ed energetico)

PLOT TOTALE

\end{document}
